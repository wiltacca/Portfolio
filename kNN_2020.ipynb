{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "kNN 2020.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wiltacca/Portfolio/blob/main/kNN_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HkT36i6ghCf"
      },
      "source": [
        "![NYPLogo.jpg](attachment:NYPLogo.jpg)\n",
        "\n",
        "\n",
        "# Practical: k-Nearest Neighbours\n",
        "\n",
        "\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- Learn to apply the k-Nearest Neighbours (k-NN) algorithm using Python, Numpy and Scikit-Learn.\n",
        "- Implement a k-NN algorithm to find the nearest neighbour.\n",
        "- Use Scikit-Learn in-built library to perform k-NN search.\n",
        "- Use k-NN for imputation.\n",
        "- Use k-NN for classification.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbllLVsfghCh"
      },
      "source": [
        "## Introduction\n",
        "The k-Nearest Neighbour (k-NN) is a widely used algorithm that can be applied to many different problems. We can use k-NN for classification or imputing missing values in our dataset. The idea behind the algorithm is intuitive and one that we uses instinctively - we simply assume things that are similar to each other works in a similar manner.\n",
        "\n",
        "In this practical, we will see how we can apply the k-NN algorithm to a set of data for imputing missing values and classification.\n",
        "\n",
        "We will first write the codes from scratch to have a better understanding of the algorithm and subsequently use Scikit-Learn for efficiency and easy of use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MKo7wcoghCi"
      },
      "source": [
        "## k-NN Using Python and Numpy\n",
        "\n",
        "We will now implement k-NN using Python with the help of Numpy. The implementation is to ensure that you have good grasp of the concept behind k-NN. In this implementation, we will use Euclidean distance as a measurement of the similarity among data points. To find the nearest neighbour of a data point, we will calculate the distances to all other data points and choose the one that is nearest (with the shortest distance).\n",
        "\n",
        "We assume that you have started a new notebook.\n",
        "\n",
        "## Step 1\n",
        "In the first cell, type in the following codes to import numpy and enter some testing data:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "#The variable data is a numpy array holding a 2-dimensional dataset\n",
        "data = np.array([\n",
        "    [0, 10],\n",
        "    [1, 9],\n",
        "    [2, 2],\n",
        "    [3, 5],\n",
        "    [4, 15],\n",
        "    [5, 9],\n",
        "    [7, 1],\n",
        "    [8, 8],\n",
        "    [9, 4]\n",
        "    ])\n",
        "\n",
        "#We are looking for the nearest neighbour of the search point [5, 2]\n",
        "searchpoint = np.array([[6,2]])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwbD25XaghCi"
      },
      "source": [
        "#Enter your codes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsYF7aV7ghCm"
      },
      "source": [
        "The above codes set up the data and the search point for which we want to search for the nearest neighbour.\n",
        "We can visualize the data by generating a scatter plot\n",
        "\n",
        "### Step 2\n",
        "\n",
        "Add a cell with the following codes to visualize the data using a scatter plot.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "#Create a figure\n",
        "plot.figure()\n",
        "#Set the title of the chart\n",
        "plot.title(\"Plot of X vs Y\")\n",
        "#Set the label for the x-axis\n",
        "plot.xlabel(\"x\")\n",
        "#Set the label for the y-axis\n",
        "plot.ylabel(\"y\")\n",
        "#Set the data to be plotted\n",
        "#data[:,0] means all rows in first column\n",
        "#data[:,1] means all rows in second column\n",
        "#c=”red” will print the data point using a red marker\n",
        "plot.scatter(data[:,0], data[:,1], c=\"red\")\n",
        "#Add the search point as another data to be plotted\n",
        "#marker=”x” will print the search point using a “x” symbol.\n",
        "plot.scatter(searchpoint[:, 0], searchpoint[:, 1], c=\"blue\", marker=\"x\")\n",
        "plot.show()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3KtVIEEghCn"
      },
      "source": [
        "#Create a plot here to visualize the data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bihn_ARFghCp"
      },
      "source": [
        "Execute the codes and you should see a figure as follows:\n",
        "\n",
        "![Step5.png](attachment:Step5.png)\n",
        "\n",
        "The red dots are our data points and the blue “x” symbol is our search point. By visual inspection, we know that [7,1] should be our nearest neighbour.\n",
        "\n",
        "Let us get the machine to do the searching for us using a k-NN algorithm.\n",
        "\n",
        "### Step 3\n",
        "\n",
        "In a new cell, key in the following Python codes that implements a k-NN algorithm.\n",
        "\n",
        "```python\n",
        "#Calculates distances from all points to the search point\n",
        "#note that axis = 1 means sum by rows\n",
        "distances = np.sqrt(np.sum((data - searchpoint)**2, axis=1)) \n",
        "\n",
        "#The function argmin calculates the index of the array element with the \n",
        "#smallest value and in this case, the shortest distance\n",
        "index_nearest_neighbour = np.argmin(distances)\n",
        "\n",
        "#Print out the nearest neighbour, you should get [7, 1]\n",
        "print(\"Nearest Neighbour={}\".format(data[index_nearest_neighbour, :]))\n",
        "#Print out the distance\n",
        "print(\"Distance={}\".format(distances[index_nearest_neighbour]))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AomrFt84ghCp"
      },
      "source": [
        "#Enter codes for k-NN here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFbeCDXUghCr"
      },
      "source": [
        "Read the comments carefully, they provide  explanations for each of the steps.\n",
        "\n",
        "### Step 4\n",
        "\n",
        "Run the codes by clicking on the Cell->Run Cells command. You should get the nearest neighbour as [7, 1] with a distance value of 1.41421356.\n",
        "\n",
        "Try it out with other values to see if the implementation works as expected.\n",
        "\n",
        "## k-NN with Scikit-Learn\n",
        "\n",
        "Usually we do not have to implement algorithms ourselves, we can use open source libraries with more efficient and proven implementations. Let us now see how we can use scikit-learn to perform k-NN.\n",
        "\n",
        "### Step 5\n",
        "\n",
        "Add the following codes to a new cell:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors as knn\n",
        "\n",
        "data = np.array([\n",
        "    [0, 10],\n",
        "    [1, 9],\n",
        "    [2, 2],\n",
        "    [3, 5],\n",
        "    [4, 15],\n",
        "    [5, 9],\n",
        "    [7, 1],\n",
        "    [8, 8],\n",
        "    [9, 4]\n",
        "    ])\n",
        "\n",
        "searchpoint = np.array([[6,2]])\n",
        "\n",
        "#As in standard scikit-learn operations, we first call the fit function\n",
        "#with our data. The number 1 means we are only looking for 1 neighbour. Also\n",
        "#See the next section for explanation on the algorithm=”brute” parameter\n",
        "model = knn(n_neighbors=1, algorithm=\"brute\").fit(data)\n",
        "#Call the kneighbors function to begin the search for the nearest neighbour \n",
        "distance, indices = model.kneighbors(searchpoint)\n",
        "#The function will return the index of the nearest neighbour as well as the\n",
        "#distance between the two points\n",
        "print(distance)\n",
        "print(data[indices])\n",
        "```\n",
        "\n",
        "### Step 6\n",
        "\n",
        "Again, run the cells and you should see that the answer is again [7,1] and the distance is 1.41421356.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq_P4axQghCs"
      },
      "source": [
        "#Use Scikit-learn's k-NN alogrithm\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MBAqtKRghCu"
      },
      "source": [
        "\n",
        "### Algorithm=Brute vs kd_tree vs ball_tree\n",
        "\n",
        "The algorithm parameter indicates how we want the k-NN algorithm to search for the nearest neighbour. If we specify  ```brute```, it uses brute force method by calculating the distance between the search point and all the data points as we have done previously in our own implementation. However, this method is inefficient and is computational intensive with large number of data points and features. In such cases, we should switch to either ```kd_tree``` or ```ball_tree```. \n",
        "\n",
        "We can also indicate ```auto``` for the algorithm parameter. In this case, we leave it to Scikit-Learn to figure out which is the best method to use. Note that if the algorithm parameter is not specified, it is defaulted to using ```auto```.\n",
        "Refer to http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms for a detailed discussion of the algorithms and their merits.\n",
        "\n",
        "## k-NN Imputation\n",
        "\n",
        "A common use of k-NN is for imputing missing values in a data set. The idea is that when there is a missing value in a data sample, we can replace the missing values with one from a similar sample. We find the nearest neighbour and use the value from the nearest neighbout as a substitutional value.\n",
        "\n",
        "Let us see how that works. We will be using Pandas and Scikit-Learn’s k-NN algorithm. Pandas helps us by providing a rich set of libraries for handling datasets.\n",
        "\n",
        "### Step 7\n",
        "\n",
        "Make sure that you have downloaded the _SmokersMissingValues.csv file_. Save the file in the same directory as your jupyter notebook file.\n",
        "\n",
        "### Step 8\n",
        "\n",
        "Start a new notebook or use a new cell and add the following codes:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors as knn\n",
        "\n",
        "#Read in data from the csv file using Pandas’s read_csv function\n",
        "df = pd.get_dummies(pd.read_csv(\"SmokersMissingValues.csv\", index_col=\"Year\"))\n",
        "#Print out the first 5 rows of the data\n",
        "df\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpDvZeughCu"
      },
      "source": [
        "#Enter your codes to read in SmokerMissingValues.csv here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dmBegFfghCw"
      },
      "source": [
        "You should see a list of values data with 2 missing values (indicated as NaN) in year 1988 and 2007.\n",
        "\n",
        "![MissingValues.JPG](attachment:MissingValues.JPG)\n",
        "\n",
        "We will now use k-NN imputer offered by Scikit-Learn to automatically find the nearest neighbour and use the values from the neighbour as substitution.\n",
        "\n",
        "Enter the following codes to run the k-NN imputation process:\n",
        "\n",
        "```python\n",
        "#Import the KNNImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "#Create a new Imputer with k=1\n",
        "#We will only use the nearest neighbour\n",
        "imputer = KNNImputer(n_neighbors=1)\n",
        "#The fit_transform will return a numpy array\n",
        "#with all values filled in, we replace our dataframe values with that \n",
        "#from the imputer\n",
        "df[:] = imputer.fit_transform(df)\n",
        "#print out and take a look at the result\n",
        "df.loc[[1988, 2007]]\n",
        "```\n",
        "\n",
        "*Note that k-NN imputer in Scikit-Learn is a recent addition, it is only available from version 0.22 and above]*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0DQg_9ughCx"
      },
      "source": [
        "#Enter your codes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trMdApmnghCz"
      },
      "source": [
        "You should see that the NaN values for year 1988 and 2007 has been imputed with values 29.0 and 20.0 respectively\n",
        "\n",
        "<pre>\n",
        "16 and Over \t16-24 \t25-34 \t35-49 \t50-59 \t60 and Over \tMethod_Unweighted \tMethod_Weighted\n",
        "Year \t\t\t\t\t\t\t\t\n",
        "1988 \t33.0 \t33.0 \t37.0 \t37.0 \t32.0 \t<b>29.0</b> \t1.0 \t0.0\n",
        "2007 \t22.0 \t27.0 \t29.0 \t25.0 \t<b>20.0</b> \t13.0 \t0.0 \t1.0\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np-2eIuCghC0"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use the codes below to try out interpolation using the nearest method.\n",
        "\n",
        "```python\n",
        "df = pd.get_dummies(pd.read_csv(\"SmokersMissingValues.csv\", index_col=\"Year\"))\n",
        "df = df.interpolate(method=\"nearest\")\n",
        "df.loc[[1988, 2007]]\n",
        "```\n",
        "Compare the results from using k-NN and interpolation. You should note that you may not get the same results from k-NN as they used different algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL-jSLODghC0"
      },
      "source": [
        "#Enter your codes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngYyvvcqghC2"
      },
      "source": [
        "Note:\n",
        "\n",
        "1. Before version 0.22, sckit-learn perform missing values imputation using mean, median or most frequent values but it does not support k-NN imputation. The k-NN imputation above only works for version 0.22 onwards, if you get an error, make sure to update your scikit-learn package.\n",
        "2. Alterative implementation are also available as add-on https://github.com/scikit-learn/scikit-learn/issues/2989 and fancyimpute (https://pypi.org/project/fancyimpute/) instead.\n",
        "3. Pandas also support operations to impute missing values, but it also does not support k-NN. It does support fill-forward, fill-backward and interpolation which are very useful for data that has certain trends. Alternatively, you can also replace missing values with fixed or mean values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXcq8ZuoghC3"
      },
      "source": [
        "## k-NN Classifier\n",
        "\n",
        "k-NN can also be used to implement a classifier. Since Scikit-Learn already provided us with a k-NN classifier, our job is much easier.\n",
        "We will use a simple data set ```churn_classifier.csv``` file to illustrate the use of the k-NN classifier.\n",
        "\n",
        "## Step 9\n",
        "\n",
        "Download the file ```churn_classifier.csv``` and place it in the same directory as your notebook file.\n",
        "\n",
        "## Step 10\n",
        "\n",
        "Place the following codes in a new cell. The comments provide the explanation of how the classifier is trained and used for prediction.\n",
        "\n",
        "```python\n",
        "import sklearn.datasets as datasets\n",
        "import pandas as pd\n",
        "\n",
        "#We need to import the k-NN Classifier from skleart.neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#Read in the CSV file, set the Id column as the index\n",
        "df = pd.read_csv(\"churn_classifier.csv\", index_col=\"Id\")\n",
        "#Print out the first 10 rows for visual inspection\n",
        "print(df.head(5))\n",
        "\n",
        "#Set the CHURNED column as our label (target value to be predicted)\n",
        "YTrain = df[\"CHURNED\"]\n",
        "#Remove the CHURNED column from our input variables\n",
        "XTrain = df.drop(\"CHURNED\", axis=1)\n",
        "#Create a k-NN Classifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "#Train the classifier using our training data\n",
        "classifier.fit(XTrain, YTrain)\n",
        "\n",
        "#Let us test, note that our testing data is similar to row with id 0 and 8\n",
        "#Expected result is 1 (likely) and 0 (unlikely)\n",
        "XTest = pd.DataFrame([\n",
        "[5.2, 7.5, 80.1, 0, 53, 0, 1, 1, 23000.00, 1],\n",
        "[15.0, 4.5, 30.1, 0, 35, 1, 0, 0, 90000.00, 1]\n",
        "])\n",
        "\n",
        "#Iterate through each row and do the predicton\n",
        "for index, row in XTest.iterrows():\n",
        "    if classifier.predict(row.values.reshape(1, -1))[0] == 0:\n",
        "        print(\"Customer not likely to terminate contract\")\n",
        "    else:\n",
        "        print(\"Customer likely to terminate contract\")   \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzZeifGghC3"
      },
      "source": [
        "#Enter your codes here to run Scikit-Learn k-NN classifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy4IcOTLghC5"
      },
      "source": [
        "## Step 11\n",
        "\n",
        "Execute the codes and you should see the following outputs.\n",
        "\n",
        "![Step20.png](attachment:Step20.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhzrOkzFghC5"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "The practical demonstrate the basic concept of the k-NN algorithm. We also illustrated the common application of the algorithm in imputation and classification using the tools provided by Scikit-Learn and Pandas."
      ]
    }
  ]
}