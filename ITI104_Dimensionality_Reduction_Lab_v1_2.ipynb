{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wiltacca/Portfolio/blob/main/ITI104_Dimensionality_Reduction_Lab_v1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3z7wFFiPxoX"
      },
      "source": [
        "### ITI104 Machine Learning Algorithms\n",
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Rod8c5PxoZ"
      },
      "source": [
        "## Introduction <br>\n",
        "In this practical, we will experiment with three main applications of dimensionality reduction techniques, i.e. <br>\n",
        "*  As a preprocessing step to improve the performance of model training <br>\n",
        "*  Data compression <br>\n",
        "*  Data visualization <br> <br>\n",
        "The dimensionality reduction techniques covered are: Singular Value Decomposition (Scikit-Learn's `PCA` class, NumPy's `svd()` function), Eigen analysis (NumPy's `eig()` function), and Manifold Learning (Scikit-Learn's `KernelPCA` class). <br> <br>\n",
        "We will use the Iris, MNIST, and Swiss roll datasets for the experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdcBWj6ePxoa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5vHaoewPxoc"
      },
      "source": [
        "## 1. PCA and `svd()` with Iris dataset <br>\n",
        "-  In this section, we will learn the basics of Scikit-Learn's `PCA` class and NumPy's `svd()` function <br>\n",
        "-  We will visualize the Iris dataset by reducing its dimensions from 4 to 2 and plotting it on a scatter plot <br>\n",
        "-  We will try to make some interpretation on the resulting visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-NrchKFPxoc"
      },
      "source": [
        "Load and explore the Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1TGKl4ZPxod"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "iris = sns.load_dataset('iris')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "claXWcQpPxod"
      },
      "outputs": [],
      "source": [
        "iris.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhsfGQO_Pxod"
      },
      "outputs": [],
      "source": [
        "iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8454b33Pxoe"
      },
      "outputs": [],
      "source": [
        "iris.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKiQ44vhPxoe"
      },
      "source": [
        "Take a look at how the data instances are distributed in the feature space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uycBpAbRPxoe"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(iris, hue='species')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbaYDlQIPxoe"
      },
      "source": [
        "Observe that Setosa is the most separable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK6MJvh7Pxoe"
      },
      "source": [
        "Separate the dataset into features and the label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzmyq_qVPxof"
      },
      "outputs": [],
      "source": [
        "X = iris.drop('species', axis=1)\n",
        "y = iris['species']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXJfPWqFPxof"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYnC6BHQPxof"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdjpTSuuPxof"
      },
      "source": [
        "Apply PCA to the dataset (feature columns only) to reduce its dimensions from 4 to 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfWKjYIePxof"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnjSIAfZPxog"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDbWWV-lPxog"
      },
      "outputs": [],
      "source": [
        "pca.n_components_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUdrEVWPxog"
      },
      "source": [
        "What ratio of the variance in the original data is accounted for by the first principal component? What ratio by the second?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIHmHRpyPxog"
      },
      "outputs": [],
      "source": [
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFxxWFxdPxog"
      },
      "source": [
        "In total, what is the percentage of the variance in the original data that is preserved by PCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oERDkrPnPxog"
      },
      "outputs": [],
      "source": [
        "np.sum(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm55YM9qPxoh"
      },
      "source": [
        "What are the coordinates of the two principal components?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6FXTJK4Pxoh"
      },
      "outputs": [],
      "source": [
        "pca.components_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8o_fGMtPxoh"
      },
      "source": [
        "First principal component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMeJlEJ-Pxoi"
      },
      "outputs": [],
      "source": [
        "pca.components_[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTExtV3HPxoi"
      },
      "source": [
        "Another way of reading the first principal component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUisQBQtPxoi"
      },
      "outputs": [],
      "source": [
        "pca.components_.T[ : , 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wReBHcJPxoi"
      },
      "source": [
        "Principal components are unit vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYor0lGrPxoi"
      },
      "outputs": [],
      "source": [
        "np.linalg.norm(pca.components_[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGlX-voAPxoj"
      },
      "outputs": [],
      "source": [
        "np.linalg.norm(pca.components_[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhPqzmRiPxoj"
      },
      "source": [
        "Look's look at the dataset in the 2-dimensional subspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLqiJYVyPxoj"
      },
      "outputs": [],
      "source": [
        "X_reduced.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN5MuAqYPxoj"
      },
      "outputs": [],
      "source": [
        "X_reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UavaIKAhPxoj"
      },
      "source": [
        "Pair plot of the reduced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4djBSTSTPxok"
      },
      "outputs": [],
      "source": [
        "X_reduced_df = pd.DataFrame(X_reduced, columns=[\"1st principal component\", \"2nd principal component\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkI-ai6QPxok"
      },
      "outputs": [],
      "source": [
        "X_reduced_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6EoN1unPxok"
      },
      "outputs": [],
      "source": [
        "iris_reduced = pd.concat([X_reduced_df, y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ETpjrSPxok"
      },
      "outputs": [],
      "source": [
        "iris_reduced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wun7_Q_LPxok"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(iris_reduced, hue='species')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6f2fbINPxok"
      },
      "source": [
        "Scatter plot of the reduced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07bPZRFmPxol"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(iris_reduced['1st principal component'], iris_reduced['2nd principal component'], hue=iris_reduced['species'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJT-btxBPxol"
      },
      "source": [
        "How does the Iris dataset distribution look like in the 2-dimensional space? <br>\n",
        "Are the three species well separated?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzwJEvbePxol"
      },
      "source": [
        "Instead of Scikit-Learn's `PCA` class, try the dimensionality reduction process using Numpy's `svd()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JEHyjPhPxol"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7QgXUdvPxol"
      },
      "source": [
        "## 2. PCA with MNIST dataset\n",
        "The MNIST dataset is a set of 70,000 small images (28x28) of handwritten digits, divided into 60,000 for training and 10,000 for testing. Each image is labeled with the digit that it represents. <br>\n",
        "-  In this section, we will first explore the MNIST dataset <br>\n",
        "-  We will compress the MNIST dataset by dimensionality reduction with different ratios of preserved variance. We will then try to recover the original dataset by decompressing it and examine the errors produced in the process <br>\n",
        "-  Dimensionality reduction is often used as a preprocessing step before training a machine learning model. We will examine the effect of dimensionality reduction on the performance of a classifier trained on the original dataset and the dataset with dimensions reduced <br>\n",
        "<br>\n",
        "(Some parts of the code in this section are adopted from Reference [1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwGoXrQcPxol"
      },
      "source": [
        "Load the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPzCAuyLPxom"
      },
      "outputs": [],
      "source": [
        "mnist_train = pd.read_csv(\"mnist_train.csv\")\n",
        "mnist_test = pd.read_csv(\"mnist_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjByKuYZPxom"
      },
      "outputs": [],
      "source": [
        "mnist_train.shape, mnist_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35IEdfUkPxom"
      },
      "outputs": [],
      "source": [
        "mnist_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQl0L-KBPxom"
      },
      "source": [
        "Separate the label from features as we usually do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cGxYKEhPxom"
      },
      "outputs": [],
      "source": [
        "X_train = mnist_train.drop(\"label\", axis=1)\n",
        "y_train = mnist_train[\"label\"]\n",
        "X_test = mnist_test.drop(\"label\", axis=1)\n",
        "y_test = mnist_test[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de_4wUDXPxom"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rN2wM4_Pxon"
      },
      "source": [
        "Let's explore the MNIST dataset using the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr4wML6MPxon"
      },
      "outputs": [],
      "source": [
        "digits = X_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciutXbffPxon"
      },
      "outputs": [],
      "source": [
        "digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFNr6ONnPxon"
      },
      "source": [
        "Pick a digit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ5aHQLlPxon"
      },
      "outputs": [],
      "source": [
        "some_digit = digits[5600]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Mhv5opPxon"
      },
      "outputs": [],
      "source": [
        "some_digit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1SoJuGDPxon"
      },
      "outputs": [],
      "source": [
        "def plot_digit(data):\n",
        "    image = data.reshape(28, 28)\n",
        "    plt.imshow(image, cmap = mpl.cm.binary,\n",
        "               interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvmSqcG3Pxoo"
      },
      "outputs": [],
      "source": [
        "plot_digit(some_digit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ToJd4A5Pxoo"
      },
      "source": [
        "It's a 5.  Let's verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqilIXrJPxoo"
      },
      "outputs": [],
      "source": [
        "mnist_test.iloc[5600, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYt4wM9KPxoo"
      },
      "source": [
        "View the images of some sample digits from 0 to 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf5GlkVJPxoo"
      },
      "outputs": [],
      "source": [
        "def plot_digits(instances, images_per_row=10, **options):\n",
        "    size = 28\n",
        "    images_per_row = min(len(instances), images_per_row)\n",
        "    images = [instance.reshape(size,size) for instance in instances]\n",
        "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
        "    row_images = []\n",
        "    n_empty = n_rows * images_per_row - len(instances)\n",
        "    images.append(np.zeros((size, size * n_empty)))\n",
        "    for row in range(n_rows):\n",
        "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
        "        row_images.append(np.concatenate(rimages, axis=1))\n",
        "    image = np.concatenate(row_images, axis=0)\n",
        "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QePDqHVKPxop"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "example_images = np.r_[digits[:1000:100], digits[1000:2000:100], digits[2200:3200:100], digits[3200:4200:100], digits[4200:5200:100], digits[5150:6030:95], digits[6150:7000:93], digits[7000:7850:98], digits[8000:8990:98], digits[9000::100]]\n",
        "plot_digits(example_images, images_per_row=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SU6vXf0Pxop"
      },
      "source": [
        "MNIST compression <br>\n",
        "One of the applications of dimensionality reduction is data compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PspqCSGjPxop"
      },
      "source": [
        "Combine `X_train` and `X_test` into one dataset (without label) for dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chl5tNkgPxop"
      },
      "outputs": [],
      "source": [
        "X = pd.concat([X_train, X_test], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ6vmT2nPxop"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhLs_WgEPxot"
      },
      "source": [
        "Apply PCA to reduce the dimensionality of the combined training and test sets <br>\n",
        "We want to retain at least 95% of the variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxQNBkCyPxot"
      },
      "source": [
        "We can set `n_components` to be a float between 0.0 and 1.0 to indicate the ratio of variance we wish to preserve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXUGRUkyPxot"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET1VpoySPxot"
      },
      "outputs": [],
      "source": [
        "pca.n_components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJSHZxhHPxot"
      },
      "outputs": [],
      "source": [
        "np.sum(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYjZEuDSPxou"
      },
      "source": [
        "If we know the number of dimensions to reduce to, we can just specify it directly as in the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0S9fTqiPxou"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components = 154)\n",
        "X_reduced = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4neVwjnPxou"
      },
      "outputs": [],
      "source": [
        "X_reduced.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfVY0CllPxou"
      },
      "source": [
        "With the dimensions reduced from 784 to 154, the dataset is now less than 20% of its original size while still preserving most of the variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnZeG-I7Pxou"
      },
      "source": [
        "Inverse transformation (performed by the `inverse_transform()` method) reverses the process of dimensionality reduction and \"decompresses\" the reduced dataset back to the original dimensions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty2alP6qPxou"
      },
      "outputs": [],
      "source": [
        "X_recovered = pca.inverse_transform(X_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQmlQb-DPxou"
      },
      "outputs": [],
      "source": [
        "X_reduced.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M1Zmn0OPxou"
      },
      "outputs": [],
      "source": [
        "X_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So9V78PdPxov"
      },
      "outputs": [],
      "source": [
        "X_recovered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9ZFhqrrPxov"
      },
      "outputs": [],
      "source": [
        "X_recovered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNoJ0yo2Pxov"
      },
      "source": [
        "Did we get back exactly the original X?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNq_aWBGPxov"
      },
      "source": [
        "Now, plot the compressed images of the digits that we have just plotted a few steps ago"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR_RssQTPxov"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "example_images = np.r_[X_recovered[60000:61000:100], X_recovered[61000:62000:100], X_recovered[62200:63200:100], X_recovered[63200:64200:100], X_recovered[64200:65200:100], X_recovered[65150:66030:95], X_recovered[66150:67000:93], X_recovered[67000:67850:98], X_recovered[68000:68990:98], X_recovered[69000::100]]\n",
        "plot_digits(example_images, images_per_row=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNSe88GIPxov"
      },
      "source": [
        "Any noticeable degradation in image quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_yr6wu4Pxov"
      },
      "source": [
        "Compute the reconstruction error (i.e. the mean squared distance between the original data and the reconstructed data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxlcy7VRPxov"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(X_recovered, X.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc8KGkmxPxow"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Edgvk9APxow"
      },
      "source": [
        "What if we compress the original dataset such that only 60% of the variance is preserved. What will be the resulting quality of the compressed images?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBlmvJhZPxow"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Afj51OEPxow"
      },
      "source": [
        "Train a SVM classifier (`LinearSVC()` or `SVC()`) on the original MNIST dataset <br>\n",
        "Time how long the training takes (you may use the magic function `%timeit`, e.g. `%timeit my_clf.fit(X,y)`), <br>\n",
        "and evaluate the model on the original test set <br>\n",
        "In order for the training not to take too long, reduce the training set size to 3000 and test set size to 1000 <br>\n",
        "`X_train_samp = X_train[ : : 20]` <br>\n",
        "`y_train_samp = y_train[ : : 20]` <br>\n",
        "`X_test_samp = X_test[ : : 10]` <br>\n",
        "`y_test_samp = y_test[ : : 10]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTZ6KCfjPxow"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHNvmSB_Pxow"
      },
      "source": [
        "Now, train the same classifier on the dataset with reduced dimensions (take 3000 and 1000 corresponding samples for training and test respectively) <br>\n",
        "`X_train_reduced = pca.fit_transform(X_train)` <br>\n",
        "`X_train_reduced_samp = X_train_reduced[ : : 20]` <br>\n",
        "`X_test_reduced = pca.transform(X_test)` <br>\n",
        "`X_test_reduced_samp = X_test_reduced[ : : 10]` <br>\n",
        "Take note of the training time. How does it compare to the previous classifier? <br>\n",
        "What about the performance of the new model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXAERF4SPxow"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVbSvQljPxow"
      },
      "source": [
        "## 3. Eigen analysis\n",
        "-  In this section, we will perform an eigen analysis using NumPy's `eig()` function\n",
        "-  The steps follow closely those presented in the lecture notes <br>\n",
        "(Some parts of the code in this section are adopted from References [1] and [2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n8HXt1nPxow"
      },
      "source": [
        "Build a 3D dataset for this experiment (the 3D dataset is the one shown in slide 8 of the lecture notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbRL0DHPPxox"
      },
      "outputs": [],
      "source": [
        "np.random.seed(4)\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoirh2RiPxox"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDp67IUVPxox"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wonv2jTPxox"
      },
      "source": [
        "PCA using NumPy's `eig()` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np-vFQ2-Pxox"
      },
      "source": [
        "Eigen function requires the dataset to be mean centered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4QVb1RYPxox"
      },
      "outputs": [],
      "source": [
        "X_centered = X - X.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeGQbd1dPxox"
      },
      "outputs": [],
      "source": [
        "X_centered.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY_fuoVBPxoy"
      },
      "source": [
        "Compute the covariance matrix of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q5GSdTUPxoy"
      },
      "outputs": [],
      "source": [
        "cov_mat = (X_centered).T.dot(X_centered) / (X.shape[0]-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrk4ZDFePxoy"
      },
      "outputs": [],
      "source": [
        "cov_mat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9N983clPxoy"
      },
      "source": [
        "Compute the eigenvalues and eigenvectors of the covariance matrix using `eig()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFg-WqWlPxoy"
      },
      "outputs": [],
      "source": [
        "eig_vals, eig_vecs = np.linalg.eig(cov_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgdxezbGPxoz"
      },
      "outputs": [],
      "source": [
        "eig_vals.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-O6G3rQPxoz"
      },
      "outputs": [],
      "source": [
        "eig_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u7lsjVNPxoz"
      },
      "outputs": [],
      "source": [
        "eig_vals.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzmUkh6EPxoz"
      },
      "outputs": [],
      "source": [
        "eig_vecs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_DyOkt3Pxoz"
      },
      "outputs": [],
      "source": [
        "eig_vecs  # Columns are eigen vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvsXP5LPxoz"
      },
      "source": [
        "Eigen vectors are unit vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7QJuTXtPxo0"
      },
      "outputs": [],
      "source": [
        "print([np.linalg.norm(eig_vecs[:,i]) for i in range(len(eig_vals))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swylmKoyPxo0"
      },
      "source": [
        "Use the eigenvalues to select the most important dimensions in the new data space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "956n2_kQPxo0"
      },
      "outputs": [],
      "source": [
        "# Make a list of (eigenvalue, eigenvector) tuples\n",
        "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
        "\n",
        "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "# Note that eig_vals returned by eig() function are not necessarily ordered\n",
        "eig_pairs.sort(key=lambda x: x[0], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkbJaziyPxo0"
      },
      "outputs": [],
      "source": [
        "eig_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BbZxkySPxo0"
      },
      "source": [
        "Assuming that we want to retain at least 97% of the total variance, how many dimensions do we need to keep?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftugoMRgPxo1"
      },
      "outputs": [],
      "source": [
        "# Only keep a certain number of eigenvectors\n",
        "# based on the \"explained variance ratio\"\n",
        "# which tells us how much information (variance) is explained by each eigenvector\n",
        "\n",
        "exp_var_percentage = 97  # Threshold of 97% explained variance\n",
        "\n",
        "tot = sum(eig_vals)\n",
        "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "num_vec_to_keep = 0\n",
        "\n",
        "for index, percentage in enumerate(cum_var_exp):\n",
        "  # print(index, percentage, exp_var_percentage, num_vec_to_keep)\n",
        "  if percentage > exp_var_percentage:\n",
        "    num_vec_to_keep = index + 1\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8CWn0qTPxo1"
      },
      "outputs": [],
      "source": [
        "num_vec_to_keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pXnVJKJPxo1"
      },
      "outputs": [],
      "source": [
        "var_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmmfdvrSPxo1"
      },
      "outputs": [],
      "source": [
        "cum_var_exp  # in terms of percentage, inflated by 100 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBHHrY9VPxo1"
      },
      "source": [
        "Build the projection matrix from the top two eigenvectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQarR0hSPxo1"
      },
      "outputs": [],
      "source": [
        "proj_mat = ((eig_vecs).T[:num_vec_to_keep]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO5SdxsGPxo2"
      },
      "outputs": [],
      "source": [
        "proj_mat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvrnpBpqPxo2"
      },
      "outputs": [],
      "source": [
        "proj_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9J6NaqPxo2"
      },
      "source": [
        "Project the data into the new data space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Wgel_XPxo2"
      },
      "outputs": [],
      "source": [
        "pca_data = X_centered.dot(proj_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6h92_PFPxo2"
      },
      "outputs": [],
      "source": [
        "pca_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou_IuZkiPxo3"
      },
      "outputs": [],
      "source": [
        "pca_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41BL1uD5Pxo3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puhAqcijPxo3"
      },
      "source": [
        "[OPTIONAL] Repeat the above eigen analysis using the Iris dataset <br>\n",
        "Compare the results with those obtained with the `svd()` function in Section 1 of this practical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMWrOmhzPxo3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwUA2d6hPxo3"
      },
      "source": [
        "## 4. Manifold learning\n",
        "\n",
        "*  In this section, we will experiment with Scikit-Learn's `KernelPCA` class to reduce the dimensionality of the famous Swiss roll dataset from 3 to 2 using the RBF and linear kernels. <br>\n",
        "(Some parts of the code in this section are adopted from Reference [1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6qCelXMPxo3"
      },
      "source": [
        "Load the swiss roll dataset and examine it <br>\n",
        "We get the dataset in `X`, and target values in `t`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFzVxqQVPxo3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgIjH9CPPxo3"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Gpdw-JzPxo4"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlAiGc7-Pxo4"
      },
      "outputs": [],
      "source": [
        "t.shape  # Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGPUOdv3Pxo4"
      },
      "outputs": [],
      "source": [
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GqcqxRPxo4"
      },
      "source": [
        "Perform Kernel PCA to reduce the dimenionality of the dataset <br>\n",
        "Try the RBF kernel (assuming that we have used grid search and found that 0.0433  is the best gamma value to use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZagc0m8Pxo4"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "X_preimage = rbf_pca.inverse_transform(X_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6k7bqg1Pxo4"
      },
      "outputs": [],
      "source": [
        "X_reduced.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLtsAiBePxo5"
      },
      "outputs": [],
      "source": [
        "X_reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpe-H6yWPxo5"
      },
      "source": [
        "How does the swiss roll look like now in the 2-dimensional space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPSGTUWZPxo5"
      },
      "outputs": [],
      "source": [
        "def plot_2d(data, target):\n",
        "    plt.figure(figsize=(11, 4))\n",
        "    plt.subplot(132)\n",
        "    plt.scatter(data[:, 0], data[:, 1], c=target, cmap=plt.cm.hot, marker=\"x\")\n",
        "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
        "    plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDGzk-AgPxo5"
      },
      "outputs": [],
      "source": [
        "plot_2d(X_reduced, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggK3vbPpPxo5"
      },
      "source": [
        "What is the reconstruction error?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8qe8i3_Pxo5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mean_squared_error(X, X_preimage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esRKxFlhPxo6"
      },
      "source": [
        "Now try using the linear kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SuUU6fIPxo6"
      },
      "outputs": [],
      "source": [
        "lin_pca = KernelPCA(n_components=2, kernel=\"linear\", fit_inverse_transform=True)\n",
        "X_reduced = lin_pca.fit_transform(X)\n",
        "X_preimage = lin_pca.inverse_transform(X_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIFhxoQ4Pxo6"
      },
      "outputs": [],
      "source": [
        "plot_2d(X_reduced, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fzOqDg0Pxo6"
      },
      "outputs": [],
      "source": [
        "mean_squared_error(X, X_preimage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW8la8xWPxo7"
      },
      "source": [
        "But `KernelPCA` with the linear kernel is simply equivalent to the `PCA` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPT8mh8nPxo7"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsbAhlAdPxo7"
      },
      "outputs": [],
      "source": [
        "plot_2d(X_reduced, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osrNQ6hrPxo7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arbBVIXYPxo7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JqCUqyHPxo8"
      },
      "source": [
        "#### References\n",
        "[1] A. Geron (2017), Hands-on machine learning with Scikit-Learn and TensorFlow, Chapter 8 (Oâ€™Reilly). <br>\n",
        "[2] G. Seif (2018), Principal Component Analysis: Your tutorial and code; https://towardsdatascience.com/principal-component-analysis-your-tutorial-and-code-9719d3d3f376."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pLJOo0uPxo8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvGoKQvmPxo8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "ITI104_Dimensionality_Reduction_Lab_v1.2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
